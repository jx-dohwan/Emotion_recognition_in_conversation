{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1URNcc0FULnOgdqkON7rPer1dGmiOF6mr",
      "authorship_tag": "ABX9TyPToVh9Q1v2IZgrH7wmwC3r",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jx-dohwan/Emotion_recognition_in_conversation.ipynb/blob/main/Emotion_recognition_in_conversation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j5j89T56rQ-X",
        "outputId": "5a67eff1-60de-494e-dae4-7f7d89a699d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers==4.25.1 in /usr/local/lib/python3.8/dist-packages (4.25.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers==4.25.1) (6.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers==4.25.1) (0.13.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers==4.25.1) (1.21.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers==4.25.1) (3.9.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers==4.25.1) (2022.6.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers==4.25.1) (23.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.8/dist-packages (from transformers==4.25.1) (0.12.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers==4.25.1) (2.25.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers==4.25.1) (4.64.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers==4.25.1) (4.4.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.25.1) (1.24.3)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.25.1) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.25.1) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.25.1) (2.10)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.8/dist-packages (0.0.post1)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers==4.25.1\n",
        "!pip install sklearn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "4NitMYgLr96E",
        "outputId": "768afe05-d354-4541-8295-b2c46dc47bac"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.13.1+cu116'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "transformers.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "r6Ngc4HEXzm4",
        "outputId": "2aca020a-0243-4f8a-c8bb-76b36eda74e2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'4.25.1'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dG3OdnDOsW3X",
        "outputId": "49eef072-6b3b-44b0-8eaf-3b68bbed91a9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  데이터 다운로드"
      ],
      "metadata": {
        "id": "hxqfhIkktFAI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/declare-lab/MELD.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NaHilvsqsr7P",
        "outputId": "2007122a-e9ed-468a-b6fe-f63b30d8065a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'MELD' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob # 어떤 리스트가 있는지 도출하는 것\n",
        "data_path = \"./MELD/data/MELD/*csv\"\n",
        "data_path_list = glob.glob(data_path)\n",
        "print(data_path_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0JubQmz6tXrt",
        "outputId": "45c48f1c-c788-4391-994c-bafbac496c47"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['./MELD/data/MELD/test_sent_emo.csv', './MELD/data/MELD/dev_sent_emo.csv', './MELD/data/MELD/train_sent_emo.csv']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 데이터 확인"
      ],
      "metadata": {
        "id": "g3Ignv8Ytr8T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!head -5 './MELD/data/MELD/dev_sent_emo.csv' # head는 해당 파일의 앞부분을 보겠다."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ixQfN_KLtpid",
        "outputId": "3d70bfe0-d546-4abc-814a-1c23b2f5d9bf"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sr No.,Utterance,Speaker,Emotion,Sentiment,Dialogue_ID,Utterance_ID,Season,Episode,StartTime,EndTime\r\n",
            "1,\"Oh my God, he’s lost it. He’s totally lost it.\",Phoebe,sadness,negative,0,0,4,7,\"00:20:57,256\",\"00:21:00,049\"\r\n",
            "2,What?,Monica,surprise,negative,0,1,4,7,\"00:21:01,927\",\"00:21:03,261\"\r\n",
            "3,\"Or! Or, we could go to the bank, close our accounts and cut them off at the source.\",Ross,neutral,neutral,1,0,4,4,\"00:12:24,660\",\"00:12:30,915\"\r\n",
            "4,You’re a genius!,Chandler,joy,positive,1,1,4,4,\"00:12:32,334\",\"00:12:33,960\"\r\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 출력\n",
        "import csv #csv 확장자를 읽어들임임\n",
        "for data_path in data_path_list:\n",
        "  f = open(data_path, 'r')\n",
        "  rdr = csv.reader(f)\n",
        "\n",
        "  for line in rdr:\n",
        "    print(line)\n",
        "    break\n",
        "\n",
        "  f.close()\n",
        "  break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lv-NAgOxt6gM",
        "outputId": "832e632e-45d2-4293-ee88-f2f8c120f060"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Sr No.', 'Utterance', 'Speaker', 'Emotion', 'Sentiment', 'Dialogue_ID', 'Utterance_ID', 'Season', 'Episode', 'StartTime', 'EndTime']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 세션으로 데이터 분할하기"
      ],
      "metadata": {
        "id": "K7ilfgU3uWHW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[발화1, 발화2, 발화3] --> 감정예측한다는 것은 발화3에 해당하는 감정을\n",
        "여기서 발화1, 발화2에 해당하는 감정이 있고, 이를 학습데이터로 사용해야된다.\n",
        "--> [발화1], [발화1, 발화2]\n",
        "\n",
        "//\n",
        "모델이 roberta: bidirectional\n",
        ": attention이 양방향\n",
        "\n",
        "[발화1, 발화2, 발화3]: 여기서 발화2에 해당하는 감정을 학습한다.\n",
        ": 동시에 발화1에 해당하는 감정을 학습한다."
      ],
      "metadata": {
        "id": "CVIeMuy0CJNa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 저장\n",
        "# roberta : bidirectional \n",
        "# attention이 양방향\n",
        "def split(session):\n",
        "    final_data = []\n",
        "    split_session = []\n",
        "    for line in session:\n",
        "        split_session.append(line)\n",
        "        final_data.append(split_session[:])    \n",
        "    return final_data\n",
        "    \n",
        "for data_path in data_path_list:\n",
        "    f = open(data_path, 'r')\n",
        "    rdr = csv.reader(f)# 리스트 하나씩 데이터 불러읽어 들이기\n",
        "    \n",
        "    \"\"\" 세션 데이터 저장할 것\"\"\"\n",
        "    session_dataset = []\n",
        "    session = []\n",
        "    speaker_set = []\n",
        "    \n",
        "    \"\"\" 실제 데이터 저장 방식 \"\"\"\n",
        "    pre_sess = 'start'\n",
        "    for i, line in enumerate(rdr):\n",
        "        if i == 0:\n",
        "            \"\"\" 저장할 데이터들 index 확인 \"\"\"\n",
        "            header  = line\n",
        "            utt_idx = header.index('Utterance')\n",
        "            speaker_idx = header.index('Speaker')\n",
        "            emo_idx = header.index('Emotion')\n",
        "            sess_idx = header.index('Dialogue_ID')\n",
        "        else:\n",
        "            utt = line[utt_idx]\n",
        "            speaker = line[speaker_idx]\n",
        "            \"\"\" 유니크한 스피커로 바꾸기 \"\"\"\n",
        "            if speaker in speaker_set:# 스키퍼셋안에 있는 스피커(즉 이미 있는 스피커라면)\n",
        "                uniq_speaker = speaker_set.index(speaker)# 어떤 사람이 말했는지 그 사람의 index를 uniq로 둔다.\n",
        "            else:# 처음 등장한 인물이라면\n",
        "                speaker_set.append(speaker)# 스피커셋에 스피커를 넣어주고\n",
        "                uniq_speaker = speaker_set.index(speaker)#유니크 스피커가 몇 번째 인덱스에 해당하는지 바꿔줌\n",
        "            emotion = line[emo_idx]\n",
        "            sess = line[sess_idx]\n",
        "            \n",
        "            if pre_sess == 'start' or sess == pre_sess:# sess이 pre_sess이 같거나 start일때 저장\n",
        "                session.append([uniq_speaker, utt, emotion])\n",
        "            else:#다를경우 초기화하고 다시저장?\n",
        "                \"\"\" 세션 데이터 저장 \"\"\"\n",
        "                # session_dataset.append(session)\n",
        "                session_dataset += split(session)# 서브세셔으로 분리하여 데이터로 바꿈\n",
        "                session = [[uniq_speaker, utt, emotion]]\n",
        "                speaker_set = []\n",
        "            pre_sess = sess   \n",
        "    \"\"\" 마지막 세션 저장 \"\"\"\n",
        "    # session_dataset.append(session)\n",
        "    session_dataset += split(session)\n",
        "    f.close()\n",
        "    \n",
        "    \"\"\" 데이터 분할하기 \"\"\"\n",
        "    break"
      ],
      "metadata": {
        "id": "wB2jqfpUyjFa"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "session_dataset[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bz5gG4f4EM-X",
        "outputId": "30693da5-60b4-4f5d-d177-ec0fd42f2bd6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0, 'Why do all you’re coffee mugs have numbers on the bottom?', 'surprise']]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "session_dataset[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4nP_cJ4ENLa",
        "outputId": "5f93bd92-599e-4ba3-d0ef-1474f71cab8c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0, 'Why do all you’re coffee mugs have numbers on the bottom?', 'surprise'],\n",
              " [1,\n",
              "  'Oh. That’s so Monica can keep track. That way if one on them is missing, she can be like, ‘Where’s number 27?!’',\n",
              "  'anger']]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 실제 감정인식에 맞게 데이터 분할하기(배치처리 알아보기)"
      ],
      "metadata": {
        "id": "3HLU10vxP0vT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "from torch.utils.data import Dataset\n",
        "def split(session):\n",
        "    final_data = []\n",
        "    split_session =[]\n",
        "    for line in session:\n",
        "        split_session.append(line)\n",
        "        final_data.append(split_session[:])\n",
        "    return final_data\n",
        "\n",
        "class data_loader(Dataset):\n",
        "    def __init__(self, data_path):\n",
        "        f = open(data_path, 'r')\n",
        "        rdr = csv.reader(f)\n",
        "\n",
        "        \"\"\"추가\"\"\"\n",
        "        emoSet = set()\n",
        "\n",
        "        \"\"\" 세션 데이터 저장할 것\"\"\"\n",
        "        self.session_dataset = []\n",
        "        session = []\n",
        "        speaker_set = []\n",
        "\n",
        "        \"\"\" 실제 데이터 저장 방식 \"\"\"\n",
        "        pre_sess = 'start'\n",
        "        for i, line in enumerate(rdr):\n",
        "            if i == 0:\n",
        "                \"\"\" 저장할 데이터들 index 확인 \"\"\"\n",
        "                header  = line\n",
        "                utt_idx = header.index('Utterance')\n",
        "                speaker_idx = header.index('Speaker')\n",
        "                emo_idx = header.index('Emotion')\n",
        "                sess_idx = header.index('Dialogue_ID')\n",
        "            else:\n",
        "                utt = line[utt_idx]\n",
        "                speaker = line[speaker_idx]\n",
        "                \"\"\" 유니크한 스피커로 바꾸기 \"\"\"\n",
        "                if speaker in speaker_set:# 스키퍼셋안에 있는 스피커(즉 이미 있는 스피커라면)\n",
        "                    uniq_speaker = speaker_set.index(speaker)# 어떤 사람이 말했는지 그 사람의 index를 uniq로 둔다.\n",
        "                else:# 처음 등장한 인물이라면\n",
        "                    speaker_set.append(speaker)# 스피커셋에 스피커를 넣어주고\n",
        "                    uniq_speaker = speaker_set.index(speaker)#유니크 스피커가 몇 번째 인덱스에 해당하는지 바꿔줌\n",
        "                emotion = line[emo_idx]\n",
        "                sess = line[sess_idx]\n",
        "                \n",
        "                if pre_sess == 'start' or sess == pre_sess:# sess이 pre_sess이 같거나 start일때 저장\n",
        "                    session.append([uniq_speaker, utt, emotion])\n",
        "                else:#다를경우 초기화하고 다시저장?\n",
        "                    \"\"\" 세션 데이터 저장 \"\"\"\n",
        "                    # session_dataset.append(session)\n",
        "                    self.session_dataset += split(session)# 서브세셔으로 분리하여 데이터로 바꿈\n",
        "                    session = [[uniq_speaker, utt, emotion]]\n",
        "                    speaker_set = []\n",
        "                    emoSet.add(emotion)\n",
        "                pre_sess = sess   \n",
        "        \"\"\" 마지막 세션 저장 \"\"\"\n",
        "        # session_dataset.append(session)\n",
        "        self.session_dataset += split(session)\n",
        "\n",
        "        self.emoList = sorted(emoSet) # 항상 같은 레이블 순서를 유지하기 위해\n",
        "        # self.emoList = ['anger', 'disgust', 'fear',' joy', 'neutral', 'sadness', 'surprise']\n",
        "        f.close()\n",
        "    \n",
        "    def __len__(self): # Dataset 기본적인 구성\n",
        "        return len(self.session_dataset) ## 리스트의 길이이\n",
        "\n",
        "    def __getitem__(self, idx): #기본적인 구성\n",
        "        return self.session_dataset[idx] ## 리스트중 하나를 불러와서 사용함\n",
        "\n",
        "    def collate_fn(self, sessions): # 배치를 위한 구성, sessions는 session_dataset모두 들어옴\n",
        "        '''\n",
        "            input:\n",
        "                data : [(session1),(session2),...]\n",
        "            return:\n",
        "                batch_input_tokens_pad: (B, L) apdded\n",
        "                batch_labels: (B)\n",
        "        '''\n",
        "        batch_input_token = []\n",
        "        for session in sessions:\n",
        "            input_token = \"\"\n",
        "            for line in session: # 하나의 세션을 linebyline으로 쪼개서 input_token으로 만든다.\n",
        "                speaker, utt, emotion = line\n",
        "                input_token += utt\n",
        "            batch_input_token.append(input_token)\n",
        "        return batch_input_token      "
      ],
      "metadata": {
        "id": "EMFZIAB0Epbc"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dev_dataset = data_loader('./MELD/data/MELD/dev_sent_emo.csv')\n",
        "dev_dataset[0], dev_dataset[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_cMqqPnCUGaQ",
        "outputId": "39c71618-7e05-4ef3-85a5-69c8dfee101b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([[0, 'Oh my God, he’s lost it. He’s totally lost it.', 'sadness']],\n",
              " [[0, 'Oh my God, he’s lost it. He’s totally lost it.', 'sadness'],\n",
              "  [1, 'What?', 'surprise']])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"배치결과 확인 \"\"\"\n",
        "from torch.utils.data import DataLoader\n",
        "dev_dataloader = DataLoader(dev_dataset, \n",
        "                            batch_size=3, \n",
        "                            shuffle=False,\n",
        "                            num_workers=4, # 메모리를 몇 개를 사용할지?\n",
        "                            collate_fn=dev_dataset.collate_fn)\n",
        "\n",
        "i = 0\n",
        "for data in dev_dataloader:\n",
        "    print(i, data)\n",
        "    i += 1\n",
        "    if i > 2:\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rg5guzeNWZL_",
        "outputId": "d72ad7d9-ef62-423a-d0ab-840254a9a491"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 ['Oh my God, he’s lost it. He’s totally lost it.', 'Oh my God, he’s lost it. He’s totally lost it.What?', 'Or! Or, we could go to the bank, close our accounts and cut them off at the source.']\n",
            "1 ['Or! Or, we could go to the bank, close our accounts and cut them off at the source.You’re a genius!', 'Or! Or, we could go to the bank, close our accounts and cut them off at the source.You’re a genius!Aww, man, now we won’t be bank buddies!', 'Or! Or, we could go to the bank, close our accounts and cut them off at the source.You’re a genius!Aww, man, now we won’t be bank buddies!Now, there’s two reasons.']\n",
            "2 ['Or! Or, we could go to the bank, close our accounts and cut them off at the source.You’re a genius!Aww, man, now we won’t be bank buddies!Now, there’s two reasons.Hey.', 'Or! Or, we could go to the bank, close our accounts and cut them off at the source.You’re a genius!Aww, man, now we won’t be bank buddies!Now, there’s two reasons.Hey.Hey!', 'Or! Or, we could go to the bank, close our accounts and cut them off at the source.You’re a genius!Aww, man, now we won’t be bank buddies!Now, there’s two reasons.Hey.Hey!Ohh, you guys, remember that cute client I told you about? I bit him.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 사전학습 모델들에 대한 백그라운드\n"
      ],
      "metadata": {
        "id": "K2e7JYSxXfvN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" 토크나이즈 확인하기\"\"\"\n",
        "# https://github.com/thunlp/PLMpapers\n",
        "from transformers import RobertaTokenizer\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')"
      ],
      "metadata": {
        "id": "cM_l39DiXIHq"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.cls_token, tokenizer.sep_token, tokenizer.pad_token)\n",
        "print(tokenizer.cls_token_id, tokenizer.sep_token_id, tokenizer.pad_token_id)"
      ],
      "metadata": {
        "id": "EghYCWsJXqRO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9a06a5f-d503-4364-8535-8d6842096981"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s> </s> <pad>\n",
            "0 2 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dir(tokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4TGRbo7VXfaA",
        "outputId": "c8d53641-213a-4365-91fd-857038801ced"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['SPECIAL_TOKENS_ATTRIBUTES',\n",
              " '__annotations__',\n",
              " '__call__',\n",
              " '__class__',\n",
              " '__delattr__',\n",
              " '__dict__',\n",
              " '__dir__',\n",
              " '__doc__',\n",
              " '__eq__',\n",
              " '__format__',\n",
              " '__ge__',\n",
              " '__getattribute__',\n",
              " '__gt__',\n",
              " '__hash__',\n",
              " '__init__',\n",
              " '__init_subclass__',\n",
              " '__le__',\n",
              " '__len__',\n",
              " '__lt__',\n",
              " '__module__',\n",
              " '__ne__',\n",
              " '__new__',\n",
              " '__reduce__',\n",
              " '__reduce_ex__',\n",
              " '__repr__',\n",
              " '__setattr__',\n",
              " '__sizeof__',\n",
              " '__str__',\n",
              " '__subclasshook__',\n",
              " '__weakref__',\n",
              " '_add_tokens',\n",
              " '_additional_special_tokens',\n",
              " '_auto_class',\n",
              " '_batch_encode_plus',\n",
              " '_batch_prepare_for_model',\n",
              " '_bos_token',\n",
              " '_call_one',\n",
              " '_cls_token',\n",
              " '_convert_id_to_token',\n",
              " '_convert_token_to_id',\n",
              " '_convert_token_to_id_with_added_voc',\n",
              " '_create_repo',\n",
              " '_create_trie',\n",
              " '_decode',\n",
              " '_decode_use_source_tokenizer',\n",
              " '_encode_plus',\n",
              " '_eos_token',\n",
              " '_eventual_warn_about_too_long_sequence',\n",
              " '_eventually_correct_t5_max_length',\n",
              " '_from_pretrained',\n",
              " '_get_files_timestamps',\n",
              " '_get_padding_truncation_strategies',\n",
              " '_in_target_context_manager',\n",
              " '_mask_token',\n",
              " '_pad',\n",
              " '_pad_token',\n",
              " '_pad_token_type_id',\n",
              " '_processor_class',\n",
              " '_save_pretrained',\n",
              " '_sep_token',\n",
              " '_set_processor_class',\n",
              " '_switch_to_input_mode',\n",
              " '_switch_to_target_mode',\n",
              " '_tokenize',\n",
              " '_unk_token',\n",
              " '_upload_modified_files',\n",
              " 'add_prefix_space',\n",
              " 'add_special_tokens',\n",
              " 'add_tokens',\n",
              " 'added_tokens_decoder',\n",
              " 'added_tokens_encoder',\n",
              " 'additional_special_tokens',\n",
              " 'additional_special_tokens_ids',\n",
              " 'all_special_ids',\n",
              " 'all_special_tokens',\n",
              " 'all_special_tokens_extended',\n",
              " 'as_target_tokenizer',\n",
              " 'batch_decode',\n",
              " 'batch_encode_plus',\n",
              " 'bos_token',\n",
              " 'bos_token_id',\n",
              " 'bpe',\n",
              " 'bpe_ranks',\n",
              " 'build_inputs_with_special_tokens',\n",
              " 'byte_decoder',\n",
              " 'byte_encoder',\n",
              " 'cache',\n",
              " 'clean_up_tokenization',\n",
              " 'cls_token',\n",
              " 'cls_token_id',\n",
              " 'convert_ids_to_tokens',\n",
              " 'convert_tokens_to_ids',\n",
              " 'convert_tokens_to_string',\n",
              " 'create_token_type_ids_from_sequences',\n",
              " 'decode',\n",
              " 'decoder',\n",
              " 'deprecation_warnings',\n",
              " 'encode',\n",
              " 'encode_plus',\n",
              " 'encoder',\n",
              " 'eos_token',\n",
              " 'eos_token_id',\n",
              " 'errors',\n",
              " 'from_pretrained',\n",
              " 'get_added_vocab',\n",
              " 'get_special_tokens_mask',\n",
              " 'get_vocab',\n",
              " 'init_inputs',\n",
              " 'init_kwargs',\n",
              " 'is_fast',\n",
              " 'mask_token',\n",
              " 'mask_token_id',\n",
              " 'max_len_sentences_pair',\n",
              " 'max_len_single_sentence',\n",
              " 'max_model_input_sizes',\n",
              " 'model_input_names',\n",
              " 'model_max_length',\n",
              " 'name_or_path',\n",
              " 'num_special_tokens_to_add',\n",
              " 'pad',\n",
              " 'pad_token',\n",
              " 'pad_token_id',\n",
              " 'pad_token_type_id',\n",
              " 'padding_side',\n",
              " 'pat',\n",
              " 'prepare_for_model',\n",
              " 'prepare_for_tokenization',\n",
              " 'prepare_seq2seq_batch',\n",
              " 'pretrained_init_configuration',\n",
              " 'pretrained_vocab_files_map',\n",
              " 'push_to_hub',\n",
              " 'register_for_auto_class',\n",
              " 'sanitize_special_tokens',\n",
              " 'save_pretrained',\n",
              " 'save_vocabulary',\n",
              " 'sep_token',\n",
              " 'sep_token_id',\n",
              " 'slow_tokenizer_class',\n",
              " 'special_tokens_map',\n",
              " 'special_tokens_map_extended',\n",
              " 'tokenize',\n",
              " 'tokens_trie',\n",
              " 'truncate_sequences',\n",
              " 'truncation_side',\n",
              " 'unique_no_split_tokens',\n",
              " 'unk_token',\n",
              " 'unk_token_id',\n",
              " 'verbose',\n",
              " 'vocab_files_names',\n",
              " 'vocab_size']"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.model_max_length"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vbdiTUmcXhhS",
        "outputId": "e86578b4-1bb0-4144-cbc6-2ef5b6c93980"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "512"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" 토크나이저 작동 \"\"\"\n",
        "res = tokenizer('hello. this is fastcampus')\n",
        "print(res)\n",
        "res = tokenizer.encode('hello. this is fastcampus')\n",
        "print(res)\n",
        "res = tokenizer(['hello. this is fastcampus','what are you doing'])\n",
        "print(res)\n",
        "res = tokenizer(['hello. this is fastcampus','what are you doing'], add_special_tokens=False)\n",
        "print(res)\n",
        "res = tokenizer.encode('hello. this is fastcampus', add_special_tokens=False)\n",
        "print(res)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "747Ij2QZXtRH",
        "outputId": "d282944d-fc74-4fe2-f750-0b15acd7bad7"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': [0, 42891, 4, 42, 16, 1769, 28135, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]}\n",
            "[0, 42891, 4, 42, 16, 1769, 28135, 2]\n",
            "{'input_ids': [[0, 42891, 4, 42, 16, 1769, 28135, 2], [0, 12196, 32, 47, 608, 2]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]}\n",
            "{'input_ids': [[42891, 4, 42, 16, 1769, 28135], [12196, 32, 47, 608]], 'attention_mask': [[1, 1, 1, 1, 1, 1], [1, 1, 1, 1]]}\n",
            "[42891, 4, 42, 16, 1769, 28135]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "from torch.utils.data import Dataset\n",
        "def split(session):\n",
        "    final_data = []\n",
        "    split_session =[]\n",
        "    for line in session:\n",
        "        split_session.append(line)\n",
        "        final_data.append(split_session[:])\n",
        "    return final_data\n",
        "\n",
        "class data_loader(Dataset):\n",
        "    def __init__(self, data_path):\n",
        "        f = open(data_path, 'r')\n",
        "        rdr = csv.reader(f)\n",
        "\n",
        "        \"\"\"추가\"\"\"\n",
        "        emoSet = set()\n",
        "        self.tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "\n",
        "        \"\"\" 세션 데이터 저장할 것\"\"\"\n",
        "        self.session_dataset = []\n",
        "        session = []\n",
        "        speaker_set = []\n",
        "\n",
        "        \"\"\" 실제 데이터 저장 방식 \"\"\"\n",
        "        pre_sess = 'start'\n",
        "        for i, line in enumerate(rdr):\n",
        "            if i == 0:\n",
        "                \"\"\" 저장할 데이터들 index 확인 \"\"\"\n",
        "                header  = line\n",
        "                utt_idx = header.index('Utterance')\n",
        "                speaker_idx = header.index('Speaker')\n",
        "                emo_idx = header.index('Emotion')\n",
        "                sess_idx = header.index('Dialogue_ID')\n",
        "            else:\n",
        "                utt = line[utt_idx]\n",
        "                speaker = line[speaker_idx]\n",
        "                \"\"\" 유니크한 스피커로 바꾸기 \"\"\"\n",
        "                if speaker in speaker_set:# 스키퍼셋안에 있는 스피커(즉 이미 있는 스피커라면)\n",
        "                    uniq_speaker = speaker_set.index(speaker)# 어떤 사람이 말했는지 그 사람의 index를 uniq로 둔다.\n",
        "                else:# 처음 등장한 인물이라면\n",
        "                    speaker_set.append(speaker)# 스피커셋에 스피커를 넣어주고\n",
        "                    uniq_speaker = speaker_set.index(speaker)#유니크 스피커가 몇 번째 인덱스에 해당하는지 바꿔줌\n",
        "                emotion = line[emo_idx]\n",
        "                sess = line[sess_idx]\n",
        "                \n",
        "                if pre_sess == 'start' or sess == pre_sess:# sess이 pre_sess이 같거나 start일때 저장\n",
        "                    session.append([uniq_speaker, utt, emotion])\n",
        "                else:#다를경우 초기화하고 다시저장?\n",
        "                    \"\"\" 세션 데이터 저장 \"\"\"\n",
        "                    # session_dataset.append(session)\n",
        "                    self.session_dataset += split(session)# 서브세셔으로 분리하여 데이터로 바꿈\n",
        "                    session = [[uniq_speaker, utt, emotion]]\n",
        "                    speaker_set = []\n",
        "                    emoSet.add(emotion)\n",
        "                pre_sess = sess   \n",
        "        \"\"\" 마지막 세션 저장 \"\"\"\n",
        "        # session_dataset.append(session)\n",
        "        self.session_dataset += split(session)\n",
        "\n",
        "        #self.emoList = sorted(emoSet) # 항상 같은 레이블 순서를 유지하기 위해\n",
        "        self.emoList = ['anger', 'disgust', 'fear', 'joy', 'neutral', 'sadness', 'surprise']\n",
        "        f.close()\n",
        "    \n",
        "    def __len__(self): # Dataset 기본적인 구성\n",
        "        return len(self.session_dataset) ## 리스트의 길이이\n",
        "\n",
        "    def __getitem__(self, idx): #기본적인 구성\n",
        "        return self.session_dataset[idx] ## 리스트중 하나를 불러와서 사용함\n",
        "\n",
        "    def padding(self, batch_input_token):# batch가 1이면 필요없다./ 길이가 다른 입력을 처리할 필요가없기 때문에\n",
        "        \"\"\"추가\"\"\"\n",
        "        \"\"\" 512 토큰 길이 넘으면 잘라내기\"\"\"\n",
        "        batch_token_ids, batch_attention_masks = batch_input_token['input_ids'], batch_input_token['attention_mask'] # \n",
        "        trunc_batch_token_ids, trunc_batch_attention_masks = [], []\n",
        "        for batch_token_id, batch_attention_mask in zip(batch_token_ids, batch_attention_masks):\n",
        "            if len(batch_token_id) > self.tokenizer.model_max_length: # 512보다 길이가 긴경우우\n",
        "                trunc_batch_token_id = [batch_token_id[0]] + batch_token_id[1:][-self.tokenizer.model_max_length+1:] \n",
        "                # cls는 필수, 1부터 길이만큼인데 뒤부분부터 자름 앞부분부터 자르면 마지막 발화에 감정 예측 토큰이 사라지면 학습이 제대로 되지 않는다. \n",
        "                trunc_batch_attention_mask = [batch_attention_mask[0]] + batch_attention_mask[1:][-self.tokenizer.model_max_length+1:]\n",
        "                # attention mask도 위와 동일하게\n",
        "                trunc_batch_token_ids.append(trunc_batch_token_id)\n",
        "                trunc_batch_attention_masks.append(trunc_batch_attention_mask)\n",
        "            else: # 512보다 길이가 짧은 경우 -> 그냥 들어간다.\n",
        "                trunc_batch_token_ids.append(batch_token_id)\n",
        "                trunc_batch_attention_masks.append(batch_attention_mask)\n",
        "        \"\"\" padding token으로 패딩하기\"\"\"\n",
        "        #\n",
        "        max_length = max([len(x) for x in trunc_batch_token_ids])\n",
        "        padding_tokens, padding_attention_masks = [], []\n",
        "        for batch_token_id, batch_attention_mask in zip(trunc_batch_token_ids, trunc_batch_attention_masks):\n",
        "            padding_tokens.append(batch_token_id + [self.tokenizer.pad_token_id for _ in range(max_length-len(batch_token_id))])\n",
        "            # 512-max_length해서 남는 부분을 padding 토큰 입력\n",
        "            padding_attention_masks.append(batch_attention_mask + [0 for _ in range(max_length-len(batch_token_id))])\n",
        "            # 0을 넣어준다.\n",
        "        return torch.tensor(padding_tokens), torch.tensor(padding_attention_masks) # 리스트가 텐서포 변함\n",
        "\n",
        "    def collate_fn(self, sessions): # 배치를 위한 구성, sessions는 session_dataset모두 들어옴\n",
        "        '''\n",
        "            input:\n",
        "                data : [(session1),(session2),...]\n",
        "            return:\n",
        "                batch_input_tokens_pad: (B, L) apdded\n",
        "                batch_labels: (B)\n",
        "        '''\n",
        "        # 컨텍스트 길이 조정해도 된다.\n",
        "        # 발화 1이런식으로 앞에 제거할 수 있다. \n",
        "        # 앞을 제구하는 이유는 뒤에 내용이 더 중요하기 때문이다.\n",
        "        \"\"\"추가\"\"\"\n",
        "        batch_input, batch_labels = [], [] # com 입력, 학습할 레이블\n",
        "        batch_PM_input = [] # pm 입력\n",
        "        for session in sessions: # 하나의 세션을 linebyline으로 쪼개서 input_token으로 만든다.\n",
        "            input_str = self.tokenizer.cls_token # 입력의 맨 앞에 들어감\n",
        "            \"\"\"For PM\"\"\"\n",
        "            current_speaker, curent_utt, current_emotion = session[-1] # 마지막을 의미함/마지막 발화자를 알기 위함\n",
        "            PM_input = [] \n",
        "            for i, line in enumerate(session): # 세션에서 하나씩 부르는데 \n",
        "                speaker, utt, emotion = line\n",
        "                input_str += \" \" + utt + self.tokenizer.sep_token # 중첩해서 더해줌 구분은 sep로 \n",
        "                if i < len(session)-1 and current_speaker == speaker: # 마지막은 무조건 스피커와 커런트가 같으니까 아닌 경우에 대해 입력해줌/스피커와 커런트 스피커와 같으면 입력\n",
        "                    PM_input.append(self.tokenizer.encode(utt, add_special_tokens=True, return_tensors='pt')) \n",
        "                    # True : concat해서 저장하는 것이 아니고 하나의 입력을 바로 PM에 태우는 것이기 때문\n",
        "                    # [cls_token, tokens, sep_token]\n",
        "            \n",
        "            \"\"\"For CoM\"\"\"\n",
        "            batch_input.append(input_str)\n",
        "            batch_labels.append(self.emoList.index(emotion))\n",
        "            batch_PM_input.append(PM_input) # batch안에 batch의 개념\n",
        "        batch_input_token = self.tokenizer(batch_input, add_special_tokens=False)\n",
        "        batch_padding_token, batch_padding_attention_mask = self.padding(batch_input_token)\n",
        "\n",
        "        return batch_padding_token, batch_padding_attention_mask, batch_PM_input, torch.tensor(batch_labels)      "
      ],
      "metadata": {
        "id": "8ToMbNz8YCwM"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" 배치 결과 확인 \"\"\"\n",
        "from torch.utils.data import DataLoader\n",
        "dev_dataset = data_loader('./MELD/data/MELD/dev_sent_emo.csv')\n",
        "dev_dataloader = DataLoader(dev_dataset, batch_size=3, shuffle=False, num_workers=4, collate_fn=dev_dataset.collate_fn)\n",
        "\n",
        "for i, data in enumerate(dev_dataloader):\n",
        "    if i == 1:\n",
        "        print(data[0].shape)\n",
        "        batch_padding_token, batch_padding_attention_mask, batch_PM_input, batch_label = data\n",
        "        print(\"batch_padding_token\", batch_padding_token)\n",
        "        print(\"batch_padding_attention_mask\", batch_padding_attention_mask)\n",
        "        print(\"batch_PM_input\", batch_PM_input)\n",
        "        print(\"batch_label\", batch_label)\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cYhZipxKlnGE",
        "outputId": "197fd042-f7e4-4e20-cca3-62e94af20048"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 58])\n",
            "batch_padding_token tensor([[    0,  1793,   328,  1793,     6,    52,   115,   213,     7,     5,\n",
            "           827,     6,   593,    84,  2349,     8,   847,   106,   160,    23,\n",
            "             5,  1300,     4,     2,   370,    17,    27,   241,    10, 16333,\n",
            "           328,     2,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1],\n",
            "        [    0,  1793,   328,  1793,     6,    52,   115,   213,     7,     5,\n",
            "           827,     6,   593,    84,  2349,     8,   847,   106,   160,    23,\n",
            "             5,  1300,     4,     2,   370,    17,    27,   241,    10, 16333,\n",
            "           328,     2,    83, 33130,     6,   313,     6,   122,    52,   351,\n",
            "            17,    27,    90,    28,   827, 30489,   328,     2,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1],\n",
            "        [    0,  1793,   328,  1793,     6,    52,   115,   213,     7,     5,\n",
            "           827,     6,   593,    84,  2349,     8,   847,   106,   160,    23,\n",
            "             5,  1300,     4,     2,   370,    17,    27,   241,    10, 16333,\n",
            "           328,     2,    83, 33130,     6,   313,     6,   122,    52,   351,\n",
            "            17,    27,    90,    28,   827, 30489,   328,     2,   978,     6,\n",
            "            89,    17,    27,    29,    80,  2188,     4,     2]])\n",
            "batch_padding_attention_mask tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
            "batch_PM_input [[], [], [tensor([[    0,  1185,    17,    27,   241,    10, 16333,   328,     2]])]]\n",
            "batch_label tensor([3, 5, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 파일로 저장하기"
      ],
      "metadata": {
        "id": "1-9g86MN3Mq5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import RobertaTokenizer\n",
        "import torch"
      ],
      "metadata": {
        "id": "VDIQWlNt4EJZ"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RobertaTokenizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j3w7A3Kw7ZdZ",
        "outputId": "e7b24f1d-ae42-4ce9-ab78-473e3f740a11"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "transformers.models.roberta.tokenization_roberta.RobertaTokenizer"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!touch dataset.py\n",
        "# 코드 복사하기"
      ],
      "metadata": {
        "id": "AwLDh_0lte8u"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/')\n",
        "from dataset import data_loader\n",
        "from torch.utils.data import DataLoader\n",
        "dev_dataset = data_loader('./MELD/data/MELD/dev_sent_emo.csv')\n",
        "dev_dataloader = DataLoader(dev_dataset, batch_size=3, shuffle=False, num_workers=4, collate_fn=dev_dataset.collate_fn)\n",
        "\n",
        "for i, data in enumerate(dev_dataloader):\n",
        "    if i == 1:\n",
        "        print(data[0].shape)\n",
        "        batch_padding_token, batch_padding_attention_mask, batch_PM_input, batch_label = data\n",
        "        print(\"batch_padding_token\", batch_padding_token)\n",
        "        print(\"batch_padding_attention_mask\", batch_padding_attention_mask)\n",
        "        print(\"batch_PM_input\", batch_PM_input)\n",
        "        print(\"batch_label\", batch_label)\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "080sD5sr3aRB",
        "outputId": "ec4be01a-9b43-4b08-9864-548e2044e75f"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 58])\n",
            "batch_padding_token tensor([[    0,  1793,   328,  1793,     6,    52,   115,   213,     7,     5,\n",
            "           827,     6,   593,    84,  2349,     8,   847,   106,   160,    23,\n",
            "             5,  1300,     4,     2,   370,    17,    27,   241,    10, 16333,\n",
            "           328,     2,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1],\n",
            "        [    0,  1793,   328,  1793,     6,    52,   115,   213,     7,     5,\n",
            "           827,     6,   593,    84,  2349,     8,   847,   106,   160,    23,\n",
            "             5,  1300,     4,     2,   370,    17,    27,   241,    10, 16333,\n",
            "           328,     2,    83, 33130,     6,   313,     6,   122,    52,   351,\n",
            "            17,    27,    90,    28,   827, 30489,   328,     2,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1],\n",
            "        [    0,  1793,   328,  1793,     6,    52,   115,   213,     7,     5,\n",
            "           827,     6,   593,    84,  2349,     8,   847,   106,   160,    23,\n",
            "             5,  1300,     4,     2,   370,    17,    27,   241,    10, 16333,\n",
            "           328,     2,    83, 33130,     6,   313,     6,   122,    52,   351,\n",
            "            17,    27,    90,    28,   827, 30489,   328,     2,   978,     6,\n",
            "            89,    17,    27,    29,    80,  2188,     4,     2]])\n",
            "batch_padding_attention_mask tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
            "batch_PM_input [[], [], [tensor([[    0,  1185,    17,    27,   241,    10, 16333,   328,     2]])]]\n",
            "batch_label tensor([3, 5, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 사전학습 모델 불러와 로딩하기"
      ],
      "metadata": {
        "id": "rYpougt6_aSI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import RobertaModel\n",
        "model = RobertaModel.from_pretrained('roberta-base')\n",
        "print('')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aa6UlYiz3qUk",
        "outputId": "b6cb9db9-bae1-434c-e360-8cb112743311"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oAEf4bIjBG3V",
        "outputId": "467d56b9-344b-4fe5-ac6a-0659ea534039"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RobertaModel(\n",
              "  (embeddings): RobertaEmbeddings(\n",
              "    (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
              "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
              "    (token_type_embeddings): Embedding(1, 768)\n",
              "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (encoder): RobertaEncoder(\n",
              "    (layer): ModuleList(\n",
              "      (0): RobertaLayer(\n",
              "        (attention): RobertaAttention(\n",
              "          (self): RobertaSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): RobertaSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): RobertaIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): RobertaOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (1): RobertaLayer(\n",
              "        (attention): RobertaAttention(\n",
              "          (self): RobertaSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): RobertaSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): RobertaIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): RobertaOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (2): RobertaLayer(\n",
              "        (attention): RobertaAttention(\n",
              "          (self): RobertaSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): RobertaSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): RobertaIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): RobertaOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (3): RobertaLayer(\n",
              "        (attention): RobertaAttention(\n",
              "          (self): RobertaSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): RobertaSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): RobertaIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): RobertaOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (4): RobertaLayer(\n",
              "        (attention): RobertaAttention(\n",
              "          (self): RobertaSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): RobertaSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): RobertaIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): RobertaOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (5): RobertaLayer(\n",
              "        (attention): RobertaAttention(\n",
              "          (self): RobertaSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): RobertaSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): RobertaIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): RobertaOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (6): RobertaLayer(\n",
              "        (attention): RobertaAttention(\n",
              "          (self): RobertaSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): RobertaSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): RobertaIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): RobertaOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (7): RobertaLayer(\n",
              "        (attention): RobertaAttention(\n",
              "          (self): RobertaSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): RobertaSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): RobertaIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): RobertaOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (8): RobertaLayer(\n",
              "        (attention): RobertaAttention(\n",
              "          (self): RobertaSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): RobertaSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): RobertaIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): RobertaOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (9): RobertaLayer(\n",
              "        (attention): RobertaAttention(\n",
              "          (self): RobertaSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): RobertaSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): RobertaIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): RobertaOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (10): RobertaLayer(\n",
              "        (attention): RobertaAttention(\n",
              "          (self): RobertaSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): RobertaSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): RobertaIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): RobertaOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (11): RobertaLayer(\n",
              "        (attention): RobertaAttention(\n",
              "          (self): RobertaSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): RobertaSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): RobertaIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): RobertaOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pooler): RobertaPooler(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (activation): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dir(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fi70mMVVBMoe",
        "outputId": "a4e87548-3d30-4c86-d9e5-6ffdbaea38e7"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['T_destination',\n",
              " '__annotations__',\n",
              " '__call__',\n",
              " '__class__',\n",
              " '__delattr__',\n",
              " '__dict__',\n",
              " '__dir__',\n",
              " '__doc__',\n",
              " '__eq__',\n",
              " '__format__',\n",
              " '__ge__',\n",
              " '__getattr__',\n",
              " '__getattribute__',\n",
              " '__gt__',\n",
              " '__hash__',\n",
              " '__init__',\n",
              " '__init_subclass__',\n",
              " '__le__',\n",
              " '__lt__',\n",
              " '__module__',\n",
              " '__ne__',\n",
              " '__new__',\n",
              " '__reduce__',\n",
              " '__reduce_ex__',\n",
              " '__repr__',\n",
              " '__setattr__',\n",
              " '__setstate__',\n",
              " '__sizeof__',\n",
              " '__str__',\n",
              " '__subclasshook__',\n",
              " '__weakref__',\n",
              " '_apply',\n",
              " '_auto_class',\n",
              " '_backward_compatibility_gradient_checkpointing',\n",
              " '_backward_hooks',\n",
              " '_buffers',\n",
              " '_call_impl',\n",
              " '_can_retrieve_inputs_from_name',\n",
              " '_convert_head_mask_to_5d',\n",
              " '_create_repo',\n",
              " '_expand_inputs_for_generation',\n",
              " '_extract_past_from_model_output',\n",
              " '_forward_hooks',\n",
              " '_forward_pre_hooks',\n",
              " '_from_config',\n",
              " '_get_backward_hooks',\n",
              " '_get_decoder_start_token_id',\n",
              " '_get_files_timestamps',\n",
              " '_get_logits_processor',\n",
              " '_get_logits_warper',\n",
              " '_get_name',\n",
              " '_get_resized_embeddings',\n",
              " '_get_resized_lm_head',\n",
              " '_get_stopping_criteria',\n",
              " '_hook_rss_memory_post_forward',\n",
              " '_hook_rss_memory_pre_forward',\n",
              " '_init_weights',\n",
              " '_is_full_backward_hook',\n",
              " '_keys_to_ignore_on_load_missing',\n",
              " '_keys_to_ignore_on_load_unexpected',\n",
              " '_keys_to_ignore_on_save',\n",
              " '_load_from_state_dict',\n",
              " '_load_pretrained_model',\n",
              " '_load_pretrained_model_low_mem',\n",
              " '_load_state_dict_post_hooks',\n",
              " '_load_state_dict_pre_hooks',\n",
              " '_maybe_warn_non_full_backward_hook',\n",
              " '_merge_criteria_processor_list',\n",
              " '_modules',\n",
              " '_named_members',\n",
              " '_no_split_modules',\n",
              " '_non_persistent_buffers_set',\n",
              " '_parameters',\n",
              " '_prepare_attention_mask_for_generation',\n",
              " '_prepare_decoder_input_ids_for_generation',\n",
              " '_prepare_encoder_decoder_kwargs_for_generation',\n",
              " '_prepare_input_ids_for_generation',\n",
              " '_prepare_model_inputs',\n",
              " '_prune_heads',\n",
              " '_register_load_state_dict_pre_hook',\n",
              " '_register_state_dict_hook',\n",
              " '_reorder_cache',\n",
              " '_replicate_for_data_parallel',\n",
              " '_resize_token_embeddings',\n",
              " '_save_to_state_dict',\n",
              " '_set_default_torch_dtype',\n",
              " '_set_gradient_checkpointing',\n",
              " '_slow_forward',\n",
              " '_state_dict_hooks',\n",
              " '_tie_encoder_decoder_weights',\n",
              " '_tie_or_clone_weights',\n",
              " '_update_model_kwargs_for_generation',\n",
              " '_upload_modified_files',\n",
              " '_validate_model_class',\n",
              " '_validate_model_kwargs',\n",
              " '_version',\n",
              " 'add_memory_hooks',\n",
              " 'add_module',\n",
              " 'adjust_logits_during_generation',\n",
              " 'apply',\n",
              " 'base_model',\n",
              " 'base_model_prefix',\n",
              " 'beam_sample',\n",
              " 'beam_search',\n",
              " 'bfloat16',\n",
              " 'buffers',\n",
              " 'children',\n",
              " 'compute_transition_beam_scores',\n",
              " 'config',\n",
              " 'config_class',\n",
              " 'constrained_beam_search',\n",
              " 'contrastive_search',\n",
              " 'cpu',\n",
              " 'create_extended_attention_mask_for_decoder',\n",
              " 'cuda',\n",
              " 'device',\n",
              " 'double',\n",
              " 'dtype',\n",
              " 'dummy_inputs',\n",
              " 'dump_patches',\n",
              " 'embeddings',\n",
              " 'encoder',\n",
              " 'estimate_tokens',\n",
              " 'eval',\n",
              " 'extra_repr',\n",
              " 'float',\n",
              " 'floating_point_ops',\n",
              " 'forward',\n",
              " 'framework',\n",
              " 'from_pretrained',\n",
              " 'generate',\n",
              " 'get_buffer',\n",
              " 'get_extended_attention_mask',\n",
              " 'get_extra_state',\n",
              " 'get_head_mask',\n",
              " 'get_input_embeddings',\n",
              " 'get_memory_footprint',\n",
              " 'get_output_embeddings',\n",
              " 'get_parameter',\n",
              " 'get_position_embeddings',\n",
              " 'get_submodule',\n",
              " 'gradient_checkpointing_disable',\n",
              " 'gradient_checkpointing_enable',\n",
              " 'greedy_search',\n",
              " 'group_beam_search',\n",
              " 'half',\n",
              " 'init_weights',\n",
              " 'invert_attention_mask',\n",
              " 'ipu',\n",
              " 'is_gradient_checkpointing',\n",
              " 'is_loaded_in_8bit',\n",
              " 'is_parallelizable',\n",
              " 'load_state_dict',\n",
              " 'main_input_name',\n",
              " 'modules',\n",
              " 'name_or_path',\n",
              " 'named_buffers',\n",
              " 'named_children',\n",
              " 'named_modules',\n",
              " 'named_parameters',\n",
              " 'num_parameters',\n",
              " 'parameters',\n",
              " 'pooler',\n",
              " 'post_init',\n",
              " 'prune_heads',\n",
              " 'push_to_hub',\n",
              " 'register_backward_hook',\n",
              " 'register_buffer',\n",
              " 'register_for_auto_class',\n",
              " 'register_forward_hook',\n",
              " 'register_forward_pre_hook',\n",
              " 'register_full_backward_hook',\n",
              " 'register_load_state_dict_post_hook',\n",
              " 'register_module',\n",
              " 'register_parameter',\n",
              " 'requires_grad_',\n",
              " 'reset_memory_hooks_state',\n",
              " 'resize_position_embeddings',\n",
              " 'resize_token_embeddings',\n",
              " 'retrieve_modules_from_names',\n",
              " 'sample',\n",
              " 'save_pretrained',\n",
              " 'set_extra_state',\n",
              " 'set_input_embeddings',\n",
              " 'share_memory',\n",
              " 'state_dict',\n",
              " 'supports_gradient_checkpointing',\n",
              " 'tie_weights',\n",
              " 'to',\n",
              " 'to_empty',\n",
              " 'train',\n",
              " 'training',\n",
              " 'type',\n",
              " 'update_keys_to_ignore',\n",
              " 'warnings_issued',\n",
              " 'xpu',\n",
              " 'zero_grad']"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#model.train() # dropout의 probability을 이용한다.\n",
        "#model.eval() # dropout의 probability을 이용용한다. -> dropout 작동하지 않는다."
      ],
      "metadata": {
        "id": "kb6icyNLBbNe"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 사전학습 모델 사용해보기\n",
        "- 이 부분 너무 어렵다."
      ],
      "metadata": {
        "id": "IovIHkOeCMFS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_padding_token.shape, batch_padding_attention_mask.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dBuHSyU3B-Hl",
        "outputId": "a3eb17c6-4dbf-40eb-b6b4-be604266ae28"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([3, 58]), torch.Size([3, 58]))"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_padding_token, batch_padding_attention_mask"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r9aR36WbCuzq",
        "outputId": "e2a99aeb-485e-4056-d5ea-1e08eb17e740"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[    0,  1793,   328,  1793,     6,    52,   115,   213,     7,     5,\n",
              "            827,     6,   593,    84,  2349,     8,   847,   106,   160,    23,\n",
              "              5,  1300,     4,     2,   370,    17,    27,   241,    10, 16333,\n",
              "            328,     2,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1,     1,     1,     1,     1,     1,     1,     1],\n",
              "         [    0,  1793,   328,  1793,     6,    52,   115,   213,     7,     5,\n",
              "            827,     6,   593,    84,  2349,     8,   847,   106,   160,    23,\n",
              "              5,  1300,     4,     2,   370,    17,    27,   241,    10, 16333,\n",
              "            328,     2,    83, 33130,     6,   313,     6,   122,    52,   351,\n",
              "             17,    27,    90,    28,   827, 30489,   328,     2,     1,     1,\n",
              "              1,     1,     1,     1,     1,     1,     1,     1],\n",
              "         [    0,  1793,   328,  1793,     6,    52,   115,   213,     7,     5,\n",
              "            827,     6,   593,    84,  2349,     8,   847,   106,   160,    23,\n",
              "              5,  1300,     4,     2,   370,    17,    27,   241,    10, 16333,\n",
              "            328,     2,    83, 33130,     6,   313,     6,   122,    52,   351,\n",
              "             17,    27,    90,    28,   827, 30489,   328,     2,   978,     6,\n",
              "             89,    17,    27,    29,    80,  2188,     4,     2]]),\n",
              " tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]))"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" for CoM \"\"\"\n",
        "batch_com_out = model(input_ids=batch_padding_token, attention_mask=batch_padding_attention_mask)['last_hidden_state']\n",
        "print(batch_com_out.shape)\n",
        "batch_com_final = batch_com_out[:,0,:] # CLS 토큰의 output 가져오기 위해\n",
        "print(batch_com_final.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FFIW0XOmC1gV",
        "outputId": "5b9b1d75-59d0-4f8f-c43e-00e79ab5c1f8"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 58, 768])\n",
            "torch.Size([3, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = model(input_ids=batch_padding_token, attention_mask=batch_padding_attention_mask)\n",
        "result.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v4H_WS2pDKFK",
        "outputId": "1c0dce75-ab8e-449d-9ecb-6a1b18d1837e"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "odict_keys(['last_hidden_state', 'pooler_output'])"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "model2 = RobertaModel.from_pretrained('roberta-base')\n",
        "# 발화1: feature1 [1, 768]\n",
        "# 발화3: feature3 [1, 768]\n",
        "# 발화6에 해당하는 감정을 예측할 때 발화1, 발화3의 정보를 사용할 것\n",
        "# feature1 + feature3\n",
        "# (feature1, feature6) 어텐션 weights w1\n",
        "# (feature3, feature6) 어텐션 weights w3\n",
        "# w1*feature1 + w3*feature6\n",
        "# GRU(feature1, feature3)\n",
        "\n",
        "\"\"\" GRU 세팅 \"\"\"\n",
        "import torch.nn as nn \n",
        "hiddenDim = model2.config.hidden_size\n",
        "zero = torch.empty(2, 1, hiddenDim)\n",
        "h0 = torch.zeros_like(zero) # (num_layers * num_directions, batch, hidden_size)\n",
        "speakerGRU = nn.GRU(hiddenDim, hiddenDim, 2, dropout=0.3) # (input, hidden, num_layer) (BERT_emb, BERT_emb, num_layer)\n",
        "\n",
        "\"\"\" GRU 통과 --> PM 결과 \"\"\"\n",
        "batch_pm_gru_final = []\n",
        "for PM_inputs in batch_PM_input:\n",
        "    if PM_inputs:\n",
        "        pm_outs = []\n",
        "        for PM_input in PM_inputs:\n",
        "            pm_out = model2(PM_input)['last_hidden_state'][:,0,:] # CLS의 출력/attention에 해당하는 것을 명시하지 않고 토큰들만 넣어 pm 출력 뽑아냄 그중 CLS\n",
        "            pm_outs.append(pm_out)\n",
        "        pm_outs = torch.cat(pm_outs, 0).unsqueeze(1) # (speaker_num, batch=1, hidden_dim)로 만듬 토치텐서\n",
        "        pm_gru_outs, _ = speakerGRU(pm_outs, h0) # (speaker_num, batch=1, hidden_dim)로 만듬 토치텐서서\n",
        "        pm_gru_final = pm_gru_outs[-1,:,:] # (1, hidden_dim) 마지막것이 중요하니 해당하는 것을 가져와서 사용함\n",
        "        batch_pm_gru_final.append(pm_gru_final)\n",
        "    else:\n",
        "        batch_pm_gru_final.append(torch.zeros(1, hiddenDim)) #pm입력이 없는 경우 torch zero를 넣어준다.\n",
        "batch_pm_gru_final = torch.cat(batch_pm_gru_final, 0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fQnKBo6EDe_x",
        "outputId": "a4045a94-0474-46f5-c60c-5e638c2deb0f"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_pm_gru_final.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_fpHOPEJqnr",
        "outputId": "bd820535-682c-41ec-e080-f73fe7e1ef60"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 추가적인 layer 구성하기"
      ],
      "metadata": {
        "id": "Vekm77IoK4aG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dev_dataset.emoList"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53WB5XIaW98y",
        "outputId": "4e330add-da16-489f-ade4-1a7f65b8759e"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['anger', 'disgust', 'fear', 'joy', 'neutral', 'sadness', 'surprise']"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" score matrix \"\"\"\n",
        "clsNum = len(dev_dataset.emoList)\n",
        "W = nn.Linear(hiddenDim, clsNum)\n",
        "final_output = W(batch_com_final + batch_pm_gru_final)\n",
        "print(final_output.shape) # (B, C)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A024ip6qJt2F",
        "outputId": "e3a67b64-d3cd-4fcd-ffa7-7c395a3f28b4"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 7])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import RobertaModel\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class ERC_model(nn.Module):\n",
        "    def __init__(self, clsNum):\n",
        "        super(ERC_model, self).__init__()\n",
        "        self.com_model = RobertaModel.from_pretrained('roberta-base')\n",
        "        self.pm_model = RobertaModel.from_pretrained('roberta-base')\n",
        "        \n",
        "        \"\"\" GRU 세팅 \"\"\"\n",
        "        self.hiddenDim = self.com_model.config.hidden_size\n",
        "        zero = torch.empty(2, 1, self.hiddenDim)\n",
        "        self.h0 = torch.zeros_like(zero) # (num_layers * num_directions, batch, hidden_size)\n",
        "        self.speakerGRU = nn.GRU(self.hiddenDim, self.hiddenDim, 2, dropout=0.3) # (input, hidden, num_layer) (BERT_emb, BERT_emb, num_layer)\n",
        "        \n",
        "        \"\"\" score matrix \"\"\"\n",
        "        self.W = nn.Linear(self.hiddenDim, clsNum)\n",
        "    def forward(self, batch_padding_token, batch_padding_attention_mask, batch_PM_input):\n",
        "        \"\"\" for CoM \"\"\"\n",
        "        batch_com_out = self.com_model(input_ids=batch_padding_token, attention_mask=batch_padding_attention_mask)['last_hidden_state']\n",
        "        batch_com_final = batch_com_out[:,0,:]\n",
        "        \n",
        "        \"\"\" GRU 통과 --> PM 결과 \"\"\"\n",
        "        batch_pm_gru_final = []\n",
        "        for PM_inputs in batch_PM_input:\n",
        "            if PM_inputs:\n",
        "                pm_outs = []\n",
        "                for PM_input in PM_inputs:\n",
        "                    pm_out = self.pm_model(PM_input)['last_hidden_state'][:,0,:]\n",
        "                    pm_outs.append(pm_out)\n",
        "                pm_outs = torch.cat(pm_outs, 0).unsqueeze(1) # (speaker_num, batch=1, hidden_dim)\n",
        "                pm_gru_outs, _ = self.speakerGRU(pm_outs, self.h0) # (speaker_num, batch=1, hidden_dim)\n",
        "                pm_gru_final = pm_gru_outs[-1,:,:] # (1, hidden_dim)\n",
        "                batch_pm_gru_final.append(pm_gru_final)\n",
        "            else:\n",
        "                batch_pm_gru_final.append(torch.zeros(1, self.hiddenDim))\n",
        "        batch_pm_gru_final = torch.cat(batch_pm_gru_final, 0)        \n",
        "        \n",
        "        \"\"\" score matrix \"\"\"\n",
        "        final_output = self.W(batch_com_final + batch_pm_gru_final) # (B, C)\n",
        "        \n",
        "        return final_output"
      ],
      "metadata": {
        "id": "_ZijZwofLnX9"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clsNum = len(dev_dataset.emoList)\n",
        "model = ERC_model(clsNum)\n",
        "result = model(batch_padding_token, batch_padding_attention_mask, batch_PM_input) # forward에 들어가서 동작작\n",
        "print(result.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dl04nUwQOq5s",
        "outputId": "9d82c21d-689f-4070-ef0b-75b3658a203c"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 7])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 파일로 저장"
      ],
      "metadata": {
        "id": "Q-lFQAu7WbFn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!touch model.py\n",
        "# 코드 복사하기"
      ],
      "metadata": {
        "id": "JxUtEAEkV8oW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YkeSRiqKWhS8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}